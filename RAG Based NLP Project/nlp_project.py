# -*- coding: utf-8 -*-
"""NLP-Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wa4gc86dCnQmpkctvDOpeI8od4LhKUcZ
"""

!pip install -U langchain langchain-community sentence-transformers chromadb

#loading of the given documents
from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader


#Chris Olah's blog using a web scraper
olah_loader = WebBaseLoader("https://colah.github.io/posts/2015-08-Understanding-LSTMs/")
olah_docs = olah_loader.load()

# CMU LSTM PDF manually in Colab
from google.colab import files
uploaded = files.upload()  # upload LSTM.pdf file here , an option pops up to cho0se from local files

# Load PDF
pdf_loader = PyPDFLoader("LSTM.pdf")  # Use the uploaded file
cmu_docs = pdf_loader.load()

# Combine both document sources
all_docs = olah_docs + cmu_docs

#splitting text into smaller chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_documents(all_docs)
#we are splitting the documents to chunks of 200 each for optimized use .

"""Embeddings: Numeric vectors that represent the meaning of text.

HuggingFace : librarry for building, sharing, and deploying machine learning models
"""

#creating free version text embeddings through huggingface
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

# Create vector embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store in Chroma vector database
vector_db = Chroma.from_documents(docs, embeddings, persist_directory="./rag_db")

"""chroma : A local vector database to store and retrieve documents by similarity

"""

from transformers import pipeline

# Loading  a lightweight text generation model .
# Model used here "flan-t5-base"
rag_pipeline = pipeline("text2text-generation", model="google/flan-t5-small")

"""retriever :Searches document database for relevant chunks"""

#creating a retrievel QA  chain

from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# Wrap the pipeline into a LangChain LLM interface
local_llm = HuggingFacePipeline(pipeline=rag_pipeline)

# Created the retrieval-based QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=local_llm,
    chain_type="stuff",
    retriever=vector_db.as_retriever()
)
# A normal llm like gpt-4 doesnt know specific documents and tries to guess based on pretraining ,retrievalQA function  chain
#indexes the documents ,convenrts them into embeddings and retrieves nost relevant part .

from langchain.chains import RetrievalQAWithSourcesChain

qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(
    llm=local_llm,
    chain_type="stuff",
    retriever=vector_db.as_retriever()
)

query = "What are input and output gates in LSTMs?"
result = qa_chain_with_sources(query)
print("Answer:", result['answer'])

# more questions can be asked egs:
#what is the role of forget gate in LSTM ?
#how is lstm different than a vanilla RNN ?

""" Models used and reasoning
 1. Embedding Model- for converting text into vector form:
Model used: all-MiniLM-L6-v2
From: Hugging Face (sentence-transformers family)
Purpose: Converts the documents -Chris Olah's blog and CMU notes into numerical vectors so that the system can semantically search for relevant parts based on a user's query.

This is a lightweight, fast, and widely used model suitable for semantic search and RAG tasks.

 2. Language Model -for generating answers based on retrieved content:
Model used: flan-t5-base
From: Hugging Face -Google's FLAN-T5 series.
Purpose: This model takes the user query and the relevant context retrieved using embeddings and generates a natural-language answer.

I  initially tried HuggingFaceHub, which required an API token. Later, I  switched to loading FLAN-T5 locally (without needing tokens), so everything worked freely in Colab.
"""

